{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5b404c7",
   "metadata": {},
   "source": [
    "InfluxDB Data Validation Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af3fe14",
   "metadata": {},
   "source": [
    "Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa38195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e90145",
   "metadata": {},
   "source": [
    "Add parent directory to path to allow importing src modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512974d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.abspath('../src'))\n",
    "\n",
    "import config\n",
    "from ha_client import HAClient\n",
    "from influx_service import InfluxService"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364052e0",
   "metadata": {},
   "source": [
    "Set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec18c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1d41b1",
   "metadata": {},
   "source": [
    "Load environment variables from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a0c459",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "def get_influx_service() -> InfluxService:\n",
    "    \"\"\"Initializes and returns an InfluxService instance.\"\"\"\n",
    "    influx_url = os.getenv(\"INFLUX_URL\")\n",
    "    influx_token = os.getenv(\"INFLUX_TOKEN\")\n",
    "    influx_org = os.getenv(\"INFLUX_ORG\")\n",
    "    # InfluxService constructor does not take influx_bucket directly\n",
    "    return InfluxService(influx_url, influx_token, influx_org)\n",
    "\n",
    "def get_ha_client() -> HAClient:\n",
    "    \"\"\"Initializes and returns an HAClient instance.\"\"\"\n",
    "    hass_url = os.getenv(\"HASS_URL\")\n",
    "    hass_token = os.getenv(\"HASS_TOKEN\")\n",
    "    return HAClient(hass_url, hass_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea7c485",
   "metadata": {},
   "source": [
    "Configuration Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1ce677",
   "metadata": {},
   "source": [
    "Verify the key configuration settings for InfluxDB connection and the historical lookback window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bf96a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"INFLUX_URL: {os.getenv('INFLUX_URL')}\")\n",
    "print(f\"INFLUX_ORG: {os.getenv('INFLUX_ORG')}\")\n",
    "print(f\"INFLUX_BUCKET: {os.getenv('INFLUX_BUCKET')}\")\n",
    "print(f\"TRAINING_LOOKBACK_HOURS: {config.TRAINING_LOOKBACK_HOURS} hours\")\n",
    "\n",
    "influx_service = get_influx_service()\n",
    "ha_client = get_ha_client()\n",
    "\n",
    "end_time = datetime.now(timezone.utc)\n",
    "start_time = end_time - timedelta(hours=config.TRAINING_LOOKBACK_HOURS)\n",
    "num_steps = int((config.TRAINING_LOOKBACK_HOURS * 60) / config.HISTORY_STEP_MINUTES)\n",
    "\n",
    "print(f\"\\nQuerying data from {start_time.isoformat()} to {end_time.isoformat()} for {num_steps} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62ecdd4",
   "metadata": {},
   "source": [
    "Entity IDs to Validate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d76dff",
   "metadata": {},
   "source": [
    "These are the entity IDs that `src/physics_features.py` uses. We will check their historical data in InfluxDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15084ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_ids_to_check = [\n",
    "    config.INDOOR_TEMP_ENTITY_ID,\n",
    "    config.OUTDOOR_TEMP_ENTITY_ID,\n",
    "    config.ACTUAL_OUTLET_TEMP_ENTITY_ID,\n",
    "    config.TARGET_INDOOR_TEMP_ENTITY_ID,\n",
    "    config.DHW_STATUS_ENTITY_ID,\n",
    "    config.DISINFECTION_STATUS_ENTITY_ID,\n",
    "    config.DHW_BOOST_HEATER_STATUS_ENTITY_ID,\n",
    "    config.DEFROST_STATUS_ENTITY_ID,\n",
    "    config.PV1_POWER_ENTITY_ID,\n",
    "    config.PV2_POWER_ENTITY_ID,\n",
    "    config.PV3_POWER_ENTITY_ID,\n",
    "    config.FIREPLACE_STATUS_ENTITY_ID,\n",
    "    config.TV_STATUS_ENTITY_ID,\n",
    "    config.PV_FORECAST_ENTITY_ID\n",
    "]\n",
    "\n",
    "print(\"Entities that will be validated:\")\n",
    "for eid in entity_ids_to_check:\n",
    "    print(f\"- {eid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb969fe",
   "metadata": {},
   "source": [
    "InfluxDB Data Retrieval and Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445429db",
   "metadata": {},
   "source": [
    "This section queries InfluxDB for each specified entity and provides a summary of the retrieved data, including count, time range, and a data preview. For binary sensors, it will also show 'on' events and duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c78d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def validate_entity_data(entity_id: str):\n",
    "    print(f\"\\n--- Validating {entity_id} ---\")\n",
    "\n",
    "    # Special handling for PV forecast as it's an attribute, not a simple state\n",
    "    if entity_id == config.PV_FORECAST_ENTITY_ID:\n",
    "        print(\"PV Forecast (attributes) cannot be directly queried from InfluxDB by entity_id alone.\")\n",
    "        print(\"Its data is usually part of a specific sensor's state attributes in HA.\")\n",
    "        print(\"Please check the live HA state for this entity in the previous notebook if needed.\")\n",
    "        return\n",
    "\n",
    "    is_binary_sensor = entity_id in [config.DHW_STATUS_ENTITY_ID, config.DISINFECTION_STATUS_ENTITY_ID, config.DHW_BOOST_HEATER_STATUS_ENTITY_ID, config.DEFROST_STATUS_ENTITY_ID, config.FIREPLACE_STATUS_ENTITY_ID, config.TV_STATUS_ENTITY_ID]\n",
    "\n",
    "    agg_fn = \"max\" if is_binary_sensor else \"mean\"\n",
    "    default_val = 0.0 if is_binary_sensor else 20.0 # Default for temps, 0 for binary\n",
    "\n",
    "    try:\n",
    "        # Use fetch_history with appropriate parameters\n",
    "        history_values = influx_service.fetch_history(entity_id, num_steps, default_val, agg_fn=agg_fn)\n",
    "\n",
    "        if not history_values:\n",
    "            print(f\"No data found for {entity_id} in the last {config.TRAINING_LOOKBACK_HOURS} hours.\")\n",
    "            return\n",
    "\n",
    "        # Create a DataFrame for consistent processing\n",
    "        # Approximate timestamps for the fetched values\n",
    "        times = [end_time - timedelta(minutes=i * config.HISTORY_STEP_MINUTES) for i in range(num_steps)][::-1]\n",
    "        df = pd.DataFrame({'time': times, 'value': history_values})\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "        print(f\"Total records found: {len(df)}\")\n",
    "        print(f\"Time range: {df['time'].min()} to {df['time'].max()}\")\n",
    "        print(\"Data preview (first 5 rows):\")\n",
    "        print(df.head())\n",
    "        print(\"\\nData preview (last 5 rows):\")\n",
    "        print(df.tail())\n",
    "\n",
    "        # Add min, max, and average value\n",
    "        if not df.empty:\n",
    "            print(f\"Min value: {df['value'].min()}\")\n",
    "            print(f\"Max value: {df['value'].max()}\")\n",
    "            print(f\"Average value: {df['value'].mean()}\")\n",
    "\n",
    "        if is_binary_sensor:\n",
    "            # Filter for 'on' states (assuming 1 for on, 0 for off)\n",
    "            on_states = df[df['value'] > 0]\n",
    "\n",
    "            if not on_states.empty:\n",
    "                print(f\"Binary sensor was 'ON' for {len(on_states)} of {num_steps} periods.\")\n",
    "                # Calculate total duration it was 'on'\n",
    "                total_on_duration_seconds = len(on_states) * config.HISTORY_STEP_MINUTES * 60\n",
    "                print(f\"Approximate total 'ON' duration: {timedelta(seconds=total_on_duration_seconds)}\")\n",
    "            else:\n",
    "                print(\"Binary sensor was never 'ON' in the queried period.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {entity_id}: {e}\")\n",
    "\n",
    "\n",
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    for entity_id in entity_ids_to_check:\n",
    "        await validate_entity_data(entity_id)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af593c7e",
   "metadata": {},
   "source": [
    "Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4107a1e0",
   "metadata": {},
   "source": [
    "Based on the output above, you can now:\n",
    "1.  **Verify Entity IDs**: Ensure that the entity IDs specified in your `.env` and `config.py` accurately reflect the sensors logging data to InfluxDB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd2c503",
   "metadata": {},
   "source": [
    "2.  **Check Data Presence**: Confirm that data exists for critical entities, especially for `pv_now`, `fireplace_on`, and `tv_on` if they are being used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f4571b",
   "metadata": {},
   "source": [
    "3.  **Inspect Values**: Look at the data previews to ensure the values are reasonable and not static or erroneous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4592c2e",
   "metadata": {},
   "source": [
    "4.  **Adjust `TRAINING_LOOKBACK_HOURS`**: If data is sparse, consider if `TRAINING_LOOKBACK_HOURS` is too short."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2653f1da",
   "metadata": {},
   "source": [
    "5.  **Review Home Assistant to InfluxDB Integration**: If data is missing or incorrect, investigate your Home Assistant configuration for sending data to InfluxDB."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
