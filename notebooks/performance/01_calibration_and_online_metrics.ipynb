{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration and Online Metrics\n",
    "\n",
    "**Purpose:** Evaluate model calibration and track online performance metrics (MAE, RMSE, Bias).\n",
    "**Author:** Roo Code\n",
    "**Date:** 2026-02-16\n",
    "\n",
    "## Setup\n",
    "This notebook uses the `ml_heating` package directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Ensure project root is in path for src imports\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "# Standard Library Imports\n",
    "from src import config\n",
    "from src.analysis import DataLoader, plotting\n",
    "from src.prediction_metrics import PredictionMetrics\n",
    "\n",
    "# Configure Plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "Fetch historical data for performance evaluation. Default is the last 14 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Loader\n",
    "loader = DataLoader()\n",
    "\n",
    "# Define Time Range\n",
    "end_time = datetime.now(timezone.utc)\n",
    "start_time = end_time - timedelta(days=14)\n",
    "\n",
    "print(f\"Fetching data from {start_time} to {end_time}...\")\n",
    "\n",
    "# Fetch Data\n",
    "df = loader.fetch_training_data(\n",
    "    start_time=start_time,\n",
    "    end_time=end_time\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(df)} rows\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Metrics Calculation\n",
    "Calculate MAE, RMSE, and Bias for the prediction period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and 'outlet_temperature' in df.columns and 'ml_target_temperature' in df.columns:\n",
    "    # Assuming 'ml_target_temperature' is the prediction and 'outlet_temperature' is the actual\n",
    "    # In a real scenario, we might need to align timestamps or use a specific prediction column\n",
    "    \n",
    "    actual = df['outlet_temperature']\n",
    "    predicted = df['ml_target_temperature'] # Placeholder for prediction\n",
    "    \n",
    "    # Calculate Metrics\n",
    "    mae = np.mean(np.abs(actual - predicted))\n",
    "    rmse = np.sqrt(np.mean((actual - predicted)**2))\n",
    "    bias = np.mean(actual - predicted)\n",
    "    \n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"Bias: {bias:.4f}\")\n",
    "    \n",
    "    # Rolling Metrics\n",
    "    rolling_mae = (actual - predicted).abs().rolling(window=48).mean() # 24h window (assuming 30m steps)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    rolling_mae.plot(label='Rolling MAE (24h)')\n",
    "    plt.title('Rolling Mean Absolute Error')\n",
    "    plt.ylabel('MAE (째C)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Missing required columns for metrics calculation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Error Distribution\n",
    "Analyze the distribution of prediction errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and 'outlet_temperature' in df.columns and 'ml_target_temperature' in df.columns:\n",
    "    errors = df['outlet_temperature'] - df['ml_target_temperature']\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(errors, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "    plt.title('Prediction Error Distribution')\n",
    "    plt.xlabel('Error (째C)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.axvline(0, color='red', linestyle='--')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data for error distribution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calibration Check\n",
    "Check if the model is well-calibrated (i.e., predicted probabilities match observed frequencies).\n",
    "For regression, we check if the residuals are independent of the predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and 'outlet_temperature' in df.columns and 'ml_target_temperature' in df.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df['ml_target_temperature'], errors, alpha=0.5)\n",
    "    plt.title('Residuals vs Predicted Value')\n",
    "    plt.xlabel('Predicted Value (째C)')\n",
    "    plt.ylabel('Residuals (째C)')\n",
    "    plt.axhline(0, color='red', linestyle='--')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data for calibration check.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusions\n",
    "Summarize performance findings here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}